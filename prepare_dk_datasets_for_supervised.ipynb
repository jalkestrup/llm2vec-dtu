{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dk_datasets =[\n",
    "    {\n",
    "        \"dataset_name\": \"hestenet\",\n",
    "        \"dataset_hf_path\": \"kardosdrur/hestenet-qa\",\n",
    "        \"query\": \"question\",\n",
    "        \"pos\": \"answer\",\n",
    "        \"loss\": \"multiple_negatives_ranking\",\n",
    "        \"label\": \"\"\n",
    "    },\n",
    "    {\n",
    "        \"dataset_name\": \"folketinget\",\n",
    "        \"dataset_hf_path\": \"kardosdrur/folketinget-discussions\",\n",
    "        \"query\": \"comment\",\n",
    "        \"pos\": \"response\",\n",
    "        \"loss\": \"multiple_negatives_ranking\",\n",
    "        \"label\": \"\"\n",
    "    },\n",
    "    {\n",
    "        \"dataset_name\": \"wiki_qa\",\n",
    "        \"dataset_hf_path\": \"kardosdrur/dawiki_qa_zephyr\",\n",
    "        \"query\": \"question\",\n",
    "        \"pos\": \"answer\",\n",
    "        \"loss\": \"multiple_negatives_ranking\",\n",
    "        \"label\": \"\"\n",
    "    },\n",
    "    {\n",
    "        \"dataset_name\": \"news\",\n",
    "        \"dataset_hf_path\": \"kardosdrur/danews_title_content\",\n",
    "        \"query\": \"title\",\n",
    "        \"pos\": \"content\",\n",
    "        \"loss\": \"multiple_negatives_ranking\",\n",
    "        \"label\": \"\"\n",
    "    },\n",
    "    {\n",
    "        \"dataset_name\": \"opensubtitles_da_no\",\n",
    "        \"dataset_hf_path\": \"kardosdrur/opensubtitles-no-da\",\n",
    "        \"query\": \"no\",\n",
    "        \"pos\": \"da\",\n",
    "        \"loss\": \"multiple_negatives_ranking\",\n",
    "        \"label\": \"overlap\"\n",
    "    },\n",
    "    {\n",
    "        \"dataset_name\": \"europarl\",\n",
    "        \"dataset_hf_path\": \"kardosdrur/europarl-scandinavian\",\n",
    "        \"query\": \"sv\",\n",
    "        \"pos\": \"da\",\n",
    "        \"loss\": \"multiple_negatives_ranking\",\n",
    "        \"label\": \"\"\n",
    "    },\n",
    "        \"dataset_name\": \"wiki_queries_gemma\",\n",
    "        \"dataset_hf_path\": \"DDSC/da-wikipedia-queries-gemma-processed\",\n",
    "        \"query\": \"anchor\",\n",
    "        \"pos\": \"positive\",\n",
    "        \"loss\": \"multiple_negatives_ranking\",\n",
    "        \"label\": \"\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "dk_datasets =[\n",
    "    {    \"dataset_name\": \"wiki_queries_gemma\",\n",
    "        \"dataset_hf_path\": \"DDSC/da-wikipedia-queries-gemma-processed\",\n",
    "        \"query\": \"anchor\",\n",
    "        \"pos\": \"positive\",\n",
    "        \"loss\": \"multiple_negatives_ranking\",\n",
    "        \"label\": \"\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets, Dataset, DatasetDict\n",
    "\n",
    "def create_combined_dataset(json_list):\n",
    "    combined_data = []\n",
    "\n",
    "    for entry in json_list:\n",
    "        # Attempt to load only the 'train' split; handle cases where the dataset split might not be directly specified\n",
    "        try:\n",
    "            dataset = load_dataset(entry['dataset_hf_path'], split='train')\n",
    "        except ValueError:\n",
    "            # Default to the first available split if 'train' is not explicitly available; you can handle as needed\n",
    "            dataset_dict = load_dataset(entry['dataset_hf_path'])\n",
    "            if 'train' in dataset_dict:\n",
    "                dataset = dataset_dict['train']\n",
    "            else:\n",
    "                # If 'train' is not available specifically, choose the first available\n",
    "                first_split = next(iter(dataset_dict.keys()))\n",
    "                dataset = dataset_dict[first_split]\n",
    "\n",
    "        # Select only the relevant columns and limit to 100,000 samples\n",
    "        truncated_dataset = dataset.select(range(min(100000, len(dataset)))).map(\n",
    "            lambda example: {\n",
    "                'query': example[entry['query']],\n",
    "                'pos': example[entry['pos']],\n",
    "                'dataset_name': entry['dataset_name']\n",
    "            },\n",
    "            remove_columns=dataset.column_names\n",
    "        )\n",
    "\n",
    "        # Append each processed dataset to the list\n",
    "        combined_data.append(truncated_dataset)\n",
    "\n",
    "    # Concatenate all datasets into one\n",
    "    combined_dataset = concatenate_datasets(combined_data)\n",
    "\n",
    "    return combined_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17a7fc87e1e0442787e21a1afecc14b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/653 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3421640712d0439892a1451758e1f732",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "085c59c4280c477fac7e84bfbe72b962",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/30280 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16ff848eac324ad2aa48f24ede9048ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/30280 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "combined_dataset = create_combined_dataset(dk_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['query', 'pos', 'dataset_name'],\n",
       "    num_rows: 30280\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'combined_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Push the directory with the multiple configurations to the Hub\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mcombined_dataset\u001b[49m\u001b[38;5;241m.\u001b[39mpush_to_hub(repo_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjealk/supervised-da\u001b[39m\u001b[38;5;124m\"\u001b[39m, config_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m, set_default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, commit_message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1M samples from 6 Danish datasets for supervised embedding finetuning\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'combined_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# Push the directory with the multiple configurations to the Hub\n",
    "combined_dataset.push_to_hub(repo_id=\"jealk/supervised-da\", config_name=\"default\", set_default=True, commit_message=\"1M samples from 6 Danish datasets for supervised embedding finetuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets saved in cache/echo-data as JSONL files.\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import os\n",
    "import json\n",
    "\n",
    "def save_combined_dataset_as_jsonl(combined_dataset, save_directory):\n",
    "    os.makedirs(save_directory, exist_ok=True)\n",
    "    grouped_samples = {}\n",
    "\n",
    "    for sample in combined_dataset:\n",
    "        dataset_name = sample['dataset_name']\n",
    "        \n",
    "        if dataset_name not in grouped_samples:\n",
    "            grouped_samples[dataset_name] = []\n",
    "        \n",
    "        # Prepare the sample\n",
    "        json_sample = {\n",
    "            \"query\": sample['query'],\n",
    "            \"positive\": sample['pos'],\n",
    "            \"negative\": \"\"  # Placeholder: you might have logic to set this\n",
    "                            # if there are negative examples as well\n",
    "        }\n",
    "        \n",
    "        grouped_samples[dataset_name].append(json_sample)\n",
    "    \n",
    "    for dataset_name, samples in grouped_samples.items():\n",
    "        jsonl_filepath = os.path.join(save_directory, f\"{dataset_name}.jsonl\")\n",
    "        \n",
    "        with open(jsonl_filepath, 'w', encoding='utf-8') as f:\n",
    "            for sample in samples:\n",
    "                f.write(json.dumps(sample) + \"\\n\")\n",
    "                \n",
    "    print(f\"Datasets saved in {save_directory} as JSONL files.\")\n",
    "\n",
    "# Example usage\n",
    "# Assuming `combined_dataset` is your Hugging Face dataset object after processing\n",
    "save_directory = \"cache/echo-data\"\n",
    "save_combined_dataset_as_jsonl(combined_dataset, save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gemma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
